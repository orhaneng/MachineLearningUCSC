{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import explained_variance_score,accuracy_score, r2_score\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   X  Y month  day  FFMC   DMC     DC  ISI  temp  RH  wind  rain  area\n0  7  5   mar  fri  86.2  26.2   94.3  5.1   8.2  51   6.7   0.0   0.0\n1  7  4   oct  tue  90.6  35.4  669.1  6.7  18.0  33   0.9   0.0   0.0\n2  7  4   oct  sat  90.6  43.7  686.9  6.7  14.6  33   1.3   0.0   0.0\n3  8  6   mar  fri  91.7  33.3   77.5  9.0   8.3  97   4.0   0.2   0.0\n4  8  6   mar  sun  89.3  51.3  102.2  9.6  11.4  99   1.8   0.0   0.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-e2e60c70a906>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'jan'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'feb'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mar'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'apr'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'may'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'jun'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'jul'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'aug'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sep'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'oct'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'nov'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'dec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mon'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tue'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wed'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'thu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'fri'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sun'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tf-cpu/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2688\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tf-cpu/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2693\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2695\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tf-cpu/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2485\u001b[0m         \u001b[0;34m\"\"\"Return the cached item, item represents a label indexer.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2486\u001b[0m         \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_item_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2487\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2488\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2489\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "dataset = pd.read_csv('/Users/omerorhan/Desktop/UCSC/MachineLearning/Assignments/venv/notebookfiles'\n",
    "                      '/FinalProject/forestfires.csv')\n",
    "print(dataset.head())\n",
    "\n",
    "#print(dataset.describe(include='all'))\n",
    "dataset.month.replace(('jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec'),(1,2,3,4,5,6,7,8,9,10,11,12), inplace=True)\n",
    "dataset.day.replace(('mon','tue','wed','thu','fri','sat','sun'),(1,2,3,4,5,6,7), inplace =True)\n",
    "print(type(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation of the Dataset:               X         Y     month       day      FFMC       DMC        DC  \\\nX      1.000000  0.539548 -0.065003 -0.024922 -0.021039 -0.048384 -0.085916   \nY      0.539548  1.000000 -0.066292 -0.005453 -0.046308  0.007782 -0.101178   \nmonth -0.065003 -0.066292  1.000000 -0.050837  0.291477  0.466645  0.868698   \nday   -0.024922 -0.005453 -0.050837  1.000000 -0.041068  0.062870  0.000105   \nFFMC  -0.021039 -0.046308  0.291477 -0.041068  1.000000  0.382619  0.330512   \nDMC   -0.048384  0.007782  0.466645  0.062870  0.382619  1.000000  0.682192   \nDC    -0.085916 -0.101178  0.868698  0.000105  0.330512  0.682192  1.000000   \nISI    0.006210 -0.024488  0.186597  0.032909  0.531805  0.305128  0.229154   \ntemp  -0.051258 -0.024103  0.368842  0.052190  0.431532  0.469594  0.496208   \nRH     0.085223  0.062221 -0.095280  0.092151 -0.300995  0.073795 -0.039192   \nwind   0.018798 -0.020341 -0.086368  0.032478 -0.028485 -0.105342 -0.203466   \nrain   0.065387  0.033234  0.013438 -0.048340  0.056702  0.074790  0.035861   \narea   0.063385  0.044873  0.056496  0.023226  0.040122  0.072994  0.049383   \n\n            ISI      temp        RH      wind      rain      area  \nX      0.006210 -0.051258  0.085223  0.018798  0.065387  0.063385  \nY     -0.024488 -0.024103  0.062221 -0.020341  0.033234  0.044873  \nmonth  0.186597  0.368842 -0.095280 -0.086368  0.013438  0.056496  \nday    0.032909  0.052190  0.092151  0.032478 -0.048340  0.023226  \nFFMC   0.531805  0.431532 -0.300995 -0.028485  0.056702  0.040122  \nDMC    0.305128  0.469594  0.073795 -0.105342  0.074790  0.072994  \nDC     0.229154  0.496208 -0.039192 -0.203466  0.035861  0.049383  \nISI    1.000000  0.394287 -0.132517  0.106826  0.067668  0.008258  \ntemp   0.394287  1.000000 -0.527390 -0.227116  0.069491  0.097844  \nRH    -0.132517 -0.527390  1.000000  0.069410  0.099751 -0.075519  \nwind   0.106826 -0.227116  0.069410  1.000000  0.061119  0.012317  \nrain   0.067668  0.069491  0.099751  0.061119  1.000000 -0.007366  \narea   0.008258  0.097844 -0.075519  0.012317 -0.007366  1.000000  \n"
     ]
    }
   ],
   "source": [
    "corr = dataset.corr(method='pearson')\n",
    "print(\"Correlation of the Dataset:\",corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-7f21ab37ae91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"temp\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Linear Regression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "data = dataset[\"temp\"].values\n",
    "\n",
    "X = data[:, 0:12]\n",
    "Y = data[:, 12]\n",
    "print(\"Linear Regression\")\n",
    "Lreg = LinearRegression()\n",
    "Lreg.fit(X, Y)\n",
    "prediction = Lreg.predict(X)\n",
    "score = explained_variance_score(Y, prediction)\n",
    "mae = mean_absolute_error(prediction, Y)\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(prediction, Y))\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(\"coefficent\")\n",
    "print(Lreg.coef_)\n",
    "\"\"\"\n",
    "Explained variance regression score function\n",
    "http://www.enlistq.com/top-5-metrics-evaluating-regression-models/\n",
    "Explained Variance Score (EVS)\n",
    "As the name implies, EVS is a metric for calculating the ratio between variance of error and variance of true values. Alternatively, this score measures how well our model can explain variations in our dataset. \n",
    "Mathematically, it can be calculated using this formula:\n",
    "where y is the true value and \\hat{y}  is the predicted value.\n",
    "\n",
    "As evident by the formula, highest value your model can achieve is 1.0.\n",
    "\n",
    "Mean Absolute Error (MAE)\n",
    "MAE is a very widely used metric to evaluate regression models and a very simple one to understand as well.\n",
    "The metric is a measure of, on average, how much our predicted value can deviate from the the real value.\n",
    "The higher the metric, the higher the deviation from true value.\n",
    "It can be calculated by taking absolute values of the error (y-\\hat{y}) and then taking an average. \n",
    "Here is the formula:\n",
    "\n",
    "\n",
    "\n",
    "Here is how you can calculate it in scikit-learn:\n",
    "\n",
    "1\n",
    "2\n",
    "3\n",
    ">>> from sklearn.metrics import mean_absolute_error\n",
    ">>> mean_absolute_error(y_data, y_pred)\n",
    "0.19999999999999993\n",
    "This means that each predicted value can deviate 0.199 units from the real value. Note that the mean absolute error is in the same units as the value we are trying to predict. It is not a ratio.\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "plt.scatter(X[:,0], Y,  color='black')\n",
    "plt.plot(X[:,0], prediction, color='blue', linewidth=3)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "print(\"Score:\", score)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7., 7., 7., 8., 8., 8., 8., 8., 8., 7., 7., 7., 6., 6., 6., 6., 5.,\n       8., 6., 6., 6., 5., 7., 7., 7., 7., 7., 7., 6., 6., 6., 6., 6., 6.,\n       6., 6., 6., 7., 7., 4., 4., 4., 4., 4., 4., 5., 5., 6., 4., 4., 4.,\n       4., 4., 4., 4., 4., 4., 4., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n       2., 4., 4., 4., 5., 5., 5., 9., 9., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 6., 6., 8., 8., 8., 8., 4., 3., 3., 3., 3., 3., 3.,\n       2., 2., 2., 4., 4., 4., 4., 4., 4., 3., 3., 3., 3., 3., 3., 3., 3.,\n       3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 2., 4., 4., 4., 4., 3., 3.,\n       3., 3., 9., 1., 2., 1., 8., 1., 2., 6., 5., 8., 2., 8., 6., 9., 3.,\n       5., 1., 7., 2., 2., 2., 7., 7., 6., 6., 8., 8., 6., 6., 6., 6., 8.,\n       5., 8., 7., 4., 1., 1., 6., 2., 2., 8., 1., 8., 5., 7., 8., 2., 8.,\n       1., 6., 7., 6., 2., 2., 4., 2., 2., 6., 4., 3., 2., 1., 6., 7., 8.,\n       5., 4., 2., 7., 6., 8., 4., 7., 7., 7., 4., 4., 4., 1., 4., 6., 8.,\n       3., 4., 2., 7., 4., 7., 8., 4., 8., 4., 1., 6., 9., 4., 8., 2., 1.,\n       6., 7., 6., 4., 2., 7., 2., 3., 5., 2., 8., 3., 8., 8., 6., 7., 6.,\n       2., 8., 4., 3., 7., 2., 3., 2., 6., 4., 4., 6., 2., 3., 4., 2., 8.,\n       2., 4., 8., 4., 4., 3., 4., 4., 4., 6., 6., 3., 5., 2., 7., 7., 7.,\n       7., 2., 9., 4., 7., 8., 7., 6., 8., 8., 6., 6., 6., 3., 3., 6., 1.,\n       7., 3., 2., 2., 7., 6., 2., 6., 4., 3., 4., 4., 6., 5., 6., 1., 6.,\n       3., 6., 1., 5., 6., 6., 4., 3., 7., 4., 1., 2., 4., 7., 6., 8., 2.,\n       2., 8., 6., 8., 6., 2., 1., 5., 3., 5., 5., 4., 7., 7., 7., 4., 4.,\n       6., 6., 1., 6., 4., 7., 4., 6., 6., 4., 4., 6., 4., 7., 3., 4., 5.,\n       6., 6., 8., 2., 6., 4., 5., 8., 8., 9., 8., 2., 3., 5., 6., 7., 7.,\n       8., 1., 2., 6., 4., 4., 5., 1., 9., 9., 3., 9., 8., 2., 2., 6., 4.,\n       4., 1., 6., 7., 9., 7., 5., 8., 6., 6., 2., 2., 8., 2., 8., 1., 8.,\n       2., 8., 2., 1., 3., 7., 1., 8., 2., 1., 2., 8., 8., 2., 1., 5., 8.,\n       6., 1., 2., 5., 6., 3., 7., 7., 4., 1., 7., 4., 3., 3., 2., 1., 8.,\n       7., 2., 8., 1., 6., 6., 2., 6., 3., 6., 6., 5., 4., 8., 9., 4., 2.,\n       4., 4., 7., 7., 9., 4., 3., 8., 2., 2., 5., 5., 4., 4., 4., 4., 1.,\n       1., 6., 6., 4., 3., 6., 7., 8., 7., 4., 2., 4., 1., 1., 2., 1., 5.,\n       6., 8., 4., 2., 7., 1., 6.])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[4. 4. 2. 3. 5. 3. 7. 3. 2. 4. 2. 6. 4. 1. 2. 9. 7. 6. 4. 1. 5. 6. 1. 2.\n 4. 4. 6. 1. 1. 7. 4. 4. 2. 9. 1. 5. 3. 2. 6. 3. 2. 4. 6. 4. 4. 7. 8. 2.\n 2. 7. 8. 8. 6. 6. 7. 8. 6. 5. 8. 3. 5. 2. 5. 7. 4. 8. 4. 8. 2. 4. 1. 8.\n 6. 8. 3. 7. 8. 8. 3. 4. 7. 2. 8. 3. 3. 6. 1. 2. 5. 2. 5. 8. 7. 1. 4. 2.\n 1. 8. 2. 4. 6. 8. 2. 4. 4. 1. 4. 1. 6. 8. 3. 4. 8. 1. 2. 6. 8. 2. 3. 5.\n 4. 8. 5. 3. 3. 6. 3. 6. 7. 4. 2. 3. 2. 6. 4. 6. 6. 1. 6. 6. 9. 4. 2. 6.\n 3. 6. 6. 8. 3. 7. 1. 2. 6. 1. 7. 6. 3. 4. 5. 2. 8. 4. 1. 3. 9. 3. 1. 7.\n 6. 3. 6. 3. 7. 7. 5. 2. 1. 4. 3. 4. 2. 3. 7. 2. 5. 6. 2. 8. 1. 7. 6. 8.\n 6. 6. 8. 2. 3. 3. 8. 4. 5. 8. 7. 7. 9. 8. 9. 8. 3. 4. 3. 6. 3. 3. 4. 7.\n 5. 4. 3. 2. 1. 4. 8. 1. 4. 6. 4. 4. 2. 3. 1. 2. 8. 4. 7. 6. 6. 6. 5. 8.\n 6. 8. 3. 4. 4. 6. 9. 4. 7. 2. 4. 7. 9. 2. 6. 6. 6. 8. 8. 8. 1. 3. 7. 1.\n 7. 1. 8. 3. 4. 4. 2. 6. 6. 2. 6. 1. 2. 7. 2. 4. 2. 6. 7. 6. 8. 9. 2. 3.\n 3. 6. 8. 5. 9. 2. 6. 6. 2. 4. 4. 7. 8. 4. 4. 7. 2. 6. 2. 7. 7. 7. 4. 8.\n 1. 8. 3. 5. 4. 6. 6. 4. 1. 3. 6. 2. 7. 4. 4. 7. 7. 3. 8. 4. 2. 1. 4. 6.\n 7. 6. 2. 4. 4. 8. 2. 4. 4. 2. 4. 4. 6. 3. 1. 5. 4. 6. 6. 6. 3. 7. 6. 5.\n 7. 4. 4. 4. 5. 1. 5. 7. 7. 6. 1. 2. 8. 2. 4. 4. 6. 8. 6. 5. 6. 7. 7. 4.\n 1. 2. 2. 7. 7. 4. 8. 4. 2. 8. 4. 9. 1. 3. 4. 1. 8. 3. 4. 6. 3. 6. 6. 6.\n 4. 4. 2. 2. 2.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-4b5300cf23ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mLreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mreg\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mLreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m print(\"score:%.2f\"\n\u001b[1;32m     14\u001b[0m       \u001b[0;34m%\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tf-cpu/lib/python3.5/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mn_jobs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[0;32m--> 458\u001b[0;31m                          y_numeric=True, multi_output=True)\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tf-cpu/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    745\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    748\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/anaconda3/envs/tf-cpu/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    545\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[4. 4. 2. 3. 5. 3. 7. 3. 2. 4. 2. 6. 4. 1. 2. 9. 7. 6. 4. 1. 5. 6. 1. 2.\n 4. 4. 6. 1. 1. 7. 4. 4. 2. 9. 1. 5. 3. 2. 6. 3. 2. 4. 6. 4. 4. 7. 8. 2.\n 2. 7. 8. 8. 6. 6. 7. 8. 6. 5. 8. 3. 5. 2. 5. 7. 4. 8. 4. 8. 2. 4. 1. 8.\n 6. 8. 3. 7. 8. 8. 3. 4. 7. 2. 8. 3. 3. 6. 1. 2. 5. 2. 5. 8. 7. 1. 4. 2.\n 1. 8. 2. 4. 6. 8. 2. 4. 4. 1. 4. 1. 6. 8. 3. 4. 8. 1. 2. 6. 8. 2. 3. 5.\n 4. 8. 5. 3. 3. 6. 3. 6. 7. 4. 2. 3. 2. 6. 4. 6. 6. 1. 6. 6. 9. 4. 2. 6.\n 3. 6. 6. 8. 3. 7. 1. 2. 6. 1. 7. 6. 3. 4. 5. 2. 8. 4. 1. 3. 9. 3. 1. 7.\n 6. 3. 6. 3. 7. 7. 5. 2. 1. 4. 3. 4. 2. 3. 7. 2. 5. 6. 2. 8. 1. 7. 6. 8.\n 6. 6. 8. 2. 3. 3. 8. 4. 5. 8. 7. 7. 9. 8. 9. 8. 3. 4. 3. 6. 3. 3. 4. 7.\n 5. 4. 3. 2. 1. 4. 8. 1. 4. 6. 4. 4. 2. 3. 1. 2. 8. 4. 7. 6. 6. 6. 5. 8.\n 6. 8. 3. 4. 4. 6. 9. 4. 7. 2. 4. 7. 9. 2. 6. 6. 6. 8. 8. 8. 1. 3. 7. 1.\n 7. 1. 8. 3. 4. 4. 2. 6. 6. 2. 6. 1. 2. 7. 2. 4. 2. 6. 7. 6. 8. 9. 2. 3.\n 3. 6. 8. 5. 9. 2. 6. 6. 2. 4. 4. 7. 8. 4. 4. 7. 2. 6. 2. 7. 7. 7. 4. 8.\n 1. 8. 3. 5. 4. 6. 6. 4. 1. 3. 6. 2. 7. 4. 4. 7. 7. 3. 8. 4. 2. 1. 4. 6.\n 7. 6. 2. 4. 4. 8. 2. 4. 4. 2. 4. 4. 6. 3. 1. 5. 4. 6. 6. 6. 3. 7. 6. 5.\n 7. 4. 4. 4. 5. 1. 5. 7. 7. 6. 1. 2. 8. 2. 4. 4. 6. 8. 6. 5. 6. 7. 7. 4.\n 1. 2. 2. 7. 7. 4. 8. 4. 2. 8. 4. 9. 1. 3. 4. 1. 8. 3. 4. 6. 3. 6. 6. 6.\n 4. 4. 2. 2. 2.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "data = dataset[\"temp\"].values\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "data = dataset.values\n",
    "\n",
    "X = data[:,0]\n",
    "Y = data[:, 12]\n",
    "print(\"Linear Regression\")\n",
    "Lreg = LinearRegression()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "reg= Lreg.fit(X_train, y_train)\n",
    "print(\"score:%.2f\"\n",
    "      % reg.score(X_test, y_test)\n",
    ")\n",
    "prediction = Lreg.predict(X_test)\n",
    "#print(Lreg.coef_)\n",
    "#The mean squared error tells you how close a regression line is to a set of points.It does this by taking \n",
    "# the distances from the points to the regression line (these distances are the “errors”) and squaring \n",
    "# them. ... It's called the mean squared error as you're finding the average of a set of errors.\n",
    "\n",
    "#print(\"Mean squared error: %.2f\"\n",
    "#      % mean_squared_error(y_test, prediction))\n",
    "#I used score instead of accuracy score because it is  regression instead of classification.\n",
    "\n",
    "print(explained_variance_score(y_test, prediction))\n",
    "print('Variance score: %.2f' % r2_score(y_test, prediction))\n",
    "\n",
    "# Plot outputs\n",
    "\n",
    "'''\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(prediction.shape)\n",
    "plt.scatter(X_test[:,0], y_test,  color='black')\n",
    "plt.plot(X_test, prediction, color='blue', linewidth=3)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor\nRandom Forest Regressor\nScore: -0.030873329945873218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tf-cpu/lib/python3.5/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(\\'Random Forest Regressor\\')\\nrfreg = RandomForestRegressor()\\nrfreg.fit(X,Y)\\nprediction_rfreg = rfreg.predict(X)\\nscore = explained_variance_score(Y, prediction_rfreg)\\nmae = mean_squared_error(prediction_rfreg, Y)\\n\\nprint(\"Score:\", score)\\nprint(\"mean_squared_error:\", mae)\\nRandom Forest is a supervised learning algorithm\\nRandom forest builds multiple decision trees and merges\\n them together to get a more accurate and stable prediction.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Random Forest Regressor')\n",
    "#supervised learning algorithm\n",
    "\"\"\"\n",
    "https://towardsdatascience.com/the-random-forest-algorithm-d457d499ffcd\n",
    "https://machinelearningmastery.com/classification-versus-regression-in-machine-learning/\n",
    "To say it in simple words: Random\n",
    " forest builds multiple decision trees and merges them together to get a more accurate and stable \n",
    " prediction.\n",
    " One big advantage of random forest is, that it can be used for both classification and regression problems,\n",
    "https://www.datacamp.com/community/tutorials/random-forests-classifier-python\n",
    "\"\"\"\n",
    "print('Random Forest Regressor')\n",
    "rfreg = RandomForestRegressor()\n",
    "rfreg.fit(X_train,y_train)\n",
    "prediction_rfreg = rfreg.predict(X_test)\n",
    "score = explained_variance_score(y_test, prediction_rfreg)\n",
    "mae = mean_squared_error(prediction_rfreg, y_test)\n",
    "\n",
    "print(\"Score:\", score)\n",
    "#print(\"mean_squared_error:\", mae)\n",
    "\"\"\"\n",
    "print('Random Forest Regressor')\n",
    "rfreg = RandomForestRegressor()\n",
    "rfreg.fit(X,Y)\n",
    "prediction_rfreg = rfreg.predict(X)\n",
    "score = explained_variance_score(Y, prediction_rfreg)\n",
    "mae = mean_squared_error(prediction_rfreg, Y)\n",
    "\n",
    "print(\"Score:\", score)\n",
    "print(\"mean_squared_error:\", mae)\n",
    "Random Forest is a supervised learning algorithm\n",
    "Random forest builds multiple decision trees and merges\n",
    " them together to get a more accurate and stable prediction.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19717306384885092\n9502.163953365382\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nregressor.fit(X,Y)\\npredictions = regressor.predict(X)\\nscore = explained_variance_score(Y, predictions)\\nmae = mean_squared_error(predictions, Y)\\n\\nprint(score)\\nprint(mae)\\n#https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regressor = DecisionTreeRegressor(random_state=0)\n",
    "\n",
    "regressor.fit(X_train,y_train)\n",
    "predictions = regressor.predict(X_test)\n",
    "score = explained_variance_score(y_test, predictions)\n",
    "mae = mean_squared_error(predictions, y_test)\n",
    "\n",
    "print(score)\n",
    "print(mae)\n",
    "\"\"\"\n",
    "regressor.fit(X,Y)\n",
    "predictions = regressor.predict(X)\n",
    "score = explained_variance_score(Y, predictions)\n",
    "mae = mean_squared_error(predictions, Y)\n",
    "\n",
    "print(score)\n",
    "print(mae)\n",
    "#https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis is a difficult regression task, where the aim is to predict the burned area of forest fires, \\nin the northeast region of Portugal, by using meteorological and other data\\n\\nhttps://archive.ics.uci.edu/ml/datasets/forest+fires\\n\\nStructure of the FWI System\\nThe diagram below illustrates the components of the FWI System. Calculation of the components is based \\non consecutive daily observations of temperature, relative humidity, wind speed, and 24-hour rainfall. \\nDMC, duff moisture code; \\nFFMC, fine fuel moisture code\\nDC, drought code;\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This is a difficult regression task, where the aim is to predict the burned area of forest fires, \n",
    "in the northeast region of Portugal, by using meteorological and other data\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/forest+fires\n",
    "\n",
    "Structure of the FWI System\n",
    "The diagram below illustrates the components of the FWI System. Calculation of the components is based \n",
    "on consecutive daily observations of temperature, relative humidity, wind speed, and 24-hour rainfall. \n",
    "DMC, duff moisture code; \n",
    "FFMC, fine fuel moisture code\n",
    "DC, drought code;\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
