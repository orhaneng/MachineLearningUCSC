{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import explained_variance_score,accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   X  Y month  day  FFMC   DMC     DC  ISI  temp  RH  wind  rain  area\n0  7  5   mar  fri  86.2  26.2   94.3  5.1   8.2  51   6.7   0.0   0.0\n1  7  4   oct  tue  90.6  35.4  669.1  6.7  18.0  33   0.9   0.0   0.0\n2  7  4   oct  sat  90.6  43.7  686.9  6.7  14.6  33   1.3   0.0   0.0\n3  8  6   mar  fri  91.7  33.3   77.5  9.0   8.3  97   4.0   0.2   0.0\n4  8  6   mar  sun  89.3  51.3  102.2  9.6  11.4  99   1.8   0.0   0.0\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('/Users/omerorhan/Desktop/UCSC/MachineLearning/Assignments/venv/notebookfiles'\n",
    "                      '/FinalProject/forestfires.csv')\n",
    "print(dataset.head())\n",
    "\n",
    "#print(dataset.describe(include='all'))\n",
    "dataset.month.replace(('jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec'),(1,2,3,4,5,6,7,8,9,10,11,12), inplace=True)\n",
    "dataset.day.replace(('mon','tue','wed','thu','fri','sat','sun'),(1,2,3,4,5,6,7), inplace =True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation of the Dataset:               X         Y     month       day      FFMC       DMC        DC  \\\nX      1.000000  0.539548 -0.065003 -0.024922 -0.021039 -0.048384 -0.085916   \nY      0.539548  1.000000 -0.066292 -0.005453 -0.046308  0.007782 -0.101178   \nmonth -0.065003 -0.066292  1.000000 -0.050837  0.291477  0.466645  0.868698   \nday   -0.024922 -0.005453 -0.050837  1.000000 -0.041068  0.062870  0.000105   \nFFMC  -0.021039 -0.046308  0.291477 -0.041068  1.000000  0.382619  0.330512   \nDMC   -0.048384  0.007782  0.466645  0.062870  0.382619  1.000000  0.682192   \nDC    -0.085916 -0.101178  0.868698  0.000105  0.330512  0.682192  1.000000   \nISI    0.006210 -0.024488  0.186597  0.032909  0.531805  0.305128  0.229154   \ntemp  -0.051258 -0.024103  0.368842  0.052190  0.431532  0.469594  0.496208   \nRH     0.085223  0.062221 -0.095280  0.092151 -0.300995  0.073795 -0.039192   \nwind   0.018798 -0.020341 -0.086368  0.032478 -0.028485 -0.105342 -0.203466   \nrain   0.065387  0.033234  0.013438 -0.048340  0.056702  0.074790  0.035861   \narea   0.063385  0.044873  0.056496  0.023226  0.040122  0.072994  0.049383   \n\n            ISI      temp        RH      wind      rain      area  \nX      0.006210 -0.051258  0.085223  0.018798  0.065387  0.063385  \nY     -0.024488 -0.024103  0.062221 -0.020341  0.033234  0.044873  \nmonth  0.186597  0.368842 -0.095280 -0.086368  0.013438  0.056496  \nday    0.032909  0.052190  0.092151  0.032478 -0.048340  0.023226  \nFFMC   0.531805  0.431532 -0.300995 -0.028485  0.056702  0.040122  \nDMC    0.305128  0.469594  0.073795 -0.105342  0.074790  0.072994  \nDC     0.229154  0.496208 -0.039192 -0.203466  0.035861  0.049383  \nISI    1.000000  0.394287 -0.132517  0.106826  0.067668  0.008258  \ntemp   0.394287  1.000000 -0.527390 -0.227116  0.069491  0.097844  \nRH    -0.132517 -0.527390  1.000000  0.069410  0.099751 -0.075519  \nwind   0.106826 -0.227116  0.069410  1.000000  0.061119  0.012317  \nrain   0.067668  0.069491  0.099751  0.061119  1.000000 -0.007366  \narea   0.008258  0.097844 -0.075519  0.012317 -0.007366  1.000000  \n"
     ]
    }
   ],
   "source": [
    "corr = dataset.corr(method='pearson')\n",
    "print(\"Correlation of the Dataset:\",corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression\nMean squared error: 3947.27\n(517, 12)\n(517,)\ncoefficent\n[ 1.96527223  0.27823493  2.7379971   0.75506519 -0.06618619  0.09758667\n -0.02970081 -0.72959547  0.85659867 -0.21478604  1.18506895 -2.93577716]\nScore: 0.02397492990181449\nMean Absolute Error: 19.309939164669355\n"
     ]
    }
   ],
   "source": [
    "data = dataset.values\n",
    "\n",
    "X = data[:, 0:12]\n",
    "Y = data[:, 12]\n",
    "print(\"Linear Regression\")\n",
    "Lreg = LinearRegression()\n",
    "Lreg.fit(X, Y)\n",
    "prediction = Lreg.predict(X)\n",
    "score = explained_variance_score(Y, prediction)\n",
    "mae = mean_absolute_error(prediction, Y)\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(prediction, Y))\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(\"coefficent\")\n",
    "print(Lreg.coef_)\n",
    "\"\"\"\n",
    "Explained variance regression score function\n",
    "http://www.enlistq.com/top-5-metrics-evaluating-regression-models/\n",
    "Explained Variance Score (EVS)\n",
    "As the name implies, EVS is a metric for calculating the ratio between variance of error and variance of true values. Alternatively, this score measures how well our model can explain variations in our dataset. \n",
    "Mathematically, it can be calculated using this formula:\n",
    "where y is the true value and \\hat{y}  is the predicted value.\n",
    "\n",
    "As evident by the formula, highest value your model can achieve is 1.0.\n",
    "\n",
    "Mean Absolute Error (MAE)\n",
    "MAE is a very widely used metric to evaluate regression models and a very simple one to understand as well.\n",
    "The metric is a measure of, on average, how much our predicted value can deviate from the the real value.\n",
    "The higher the metric, the higher the deviation from true value.\n",
    "It can be calculated by taking absolute values of the error (y-\\hat{y}) and then taking an average. \n",
    "Here is the formula:\n",
    "\n",
    "\n",
    "\n",
    "Here is how you can calculate it in scikit-learn:\n",
    "\n",
    "1\n",
    "2\n",
    "3\n",
    ">>> from sklearn.metrics import mean_absolute_error\n",
    ">>> mean_absolute_error(y_data, y_pred)\n",
    "0.19999999999999993\n",
    "This means that each predicted value can deviate 0.199 units from the real value. Note that the mean absolute error is in the same units as the value we are trying to predict. It is not a ratio.\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "plt.scatter(X[:,0], Y,  color='black')\n",
    "plt.plot(X[:,0], prediction, color='blue', linewidth=3)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()\n",
    "'''\n",
    "print(\"Score:\", score)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression\n[ 2.34568693e+00 -2.14938665e-03  1.31762052e+00  9.02321556e-02\n -2.74154084e-01  1.34456440e-01 -2.28663068e-02  1.15711445e-01\n  2.63036359e-01 -2.39827472e-01  5.05456766e-01 -2.43503384e+00]\nMean squared error: 7771.67\nscore:0.00\n0.006631360013166332\n"
     ]
    }
   ],
   "source": [
    "data = dataset.values\n",
    "\n",
    "X = data[:, 0:12]\n",
    "Y = data[:, 12]\n",
    "print(\"Linear Regression\")\n",
    "Lreg = LinearRegression()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "reg= Lreg.fit(X_train, y_train)\n",
    "prediction = Lreg.predict(X_test)\n",
    "print(Lreg.coef_)\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(prediction, y_test))\n",
    "#I used score instead of accuracy score because it is  regression instead of classification.\n",
    "print(\"score:%.2f\"\n",
    "      % reg.score(X_test, y_test)\n",
    ")\n",
    "print(explained_variance_score(y_test, prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor\nRandom Forest Regressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tf-cpu/lib/python3.5/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [171, 517]",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-495441886607>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprediction_rfreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrfreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplained_variance_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_rfreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_rfreg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Score:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tf-cpu/lib/python3.5/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36mmean_absolute_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \"\"\"\n\u001b[1;32m    169\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 170\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     output_errors = np.average(np.abs(y_pred - y_true),\n",
      "\u001b[0;32m/anaconda3/envs/tf-cpu/lib/python3.5/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \"\"\"\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tf-cpu/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 230\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [171, 517]"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "print('Random Forest Regressor')\n",
    "#supervised learning algorithm\n",
    "\"\"\"\n",
    "https://towardsdatascience.com/the-random-forest-algorithm-d457d499ffcd\n",
    "https://machinelearningmastery.com/classification-versus-regression-in-machine-learning/\n",
    "To say it in simple words: Random\n",
    " forest builds multiple decision trees and merges them together to get a more accurate and stable \n",
    " prediction.\n",
    " One big advantage of random forest is, that it can be used for both classification and regression problems,\n",
    "https://www.datacamp.com/community/tutorials/random-forests-classifier-python\n",
    "\"\"\"\n",
    "print('Random Forest Regressor')\n",
    "rfreg = RandomForestRegressor()\n",
    "rfreg.fit(X,Y)\n",
    "prediction_rfreg = rfreg.predict(X)\n",
    "score = explained_variance_score(Y, prediction_rfreg)\n",
    "mae = mean_absolute_error(prediction_rfreg, Y)\n",
    "\n",
    "print(\"Score:\", score)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "\"\"\"\n",
    "Random Forest is a supervised learning algorithm\n",
    "Random forest builds multiple decision trees and merges\n",
    " them together to get a more accurate and stable prediction.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is a difficult regression task, where the aim is to predict the burned area of forest fires, \n",
    "in the northeast region of Portugal, by using meteorological and other data\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/forest+fires\n",
    "\n",
    "Structure of the FWI System\n",
    "The diagram below illustrates the components of the FWI System. Calculation of the components is based \n",
    "on consecutive daily observations of temperature, relative humidity, wind speed, and 24-hour rainfall. \n",
    "DMC, duff moisture code; \n",
    "FFMC, fine fuel moisture code\n",
    "DC, drought code;\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
